{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2C Training\n",
    "\n",
    "This is an interactive workflow for E2C training. Note that this specific case may generate results that are distinct from the paper.\n",
    "\n",
    "During the training process (while the last cell is running), you can monitor the training status with Tensorboard. Make sure `tensorboard` is installed properly. To install `tensorboard`:  \n",
    "`pip install tensorboard`  \n",
    "\n",
    "All the data used for `tensorboard` are stored in `logs/` directory. To turn on `tensorboard`:  \n",
    "`tensorboard --logdir=logs --port=5678` (`--port` is necesary for port-forwarding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Zhaoyang Larry Jin  \n",
    "Stanford University  \n",
    "zjin@stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load libaraies and config hardware (gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import e2c as e2c_util\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\", \"2\", \"3\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# GPU memory management\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.session specification\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.75\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Specify params and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### case specification ######################\n",
    "\n",
    "data_dir = '/data3/Astro/personal/zjin/datasets/9W_MS_BHP_RATE_GAU/'\n",
    "output_dir = './saved_models/'\n",
    "\n",
    "case_name = '9w_ms_bhp_rate'\n",
    "case_suffix = '_fix_wl_rel_8'\n",
    "train_suffix = '_with_p'\n",
    "model_suffix = '_flux_loss'\n",
    "\n",
    "n_train_run = 300\n",
    "n_eval_run = 100\n",
    "num_t = 20 \n",
    "dt = 100\n",
    "n_train_step = n_train_run * num_t\n",
    "n_eval_step = n_eval_run * num_t\n",
    "\n",
    "\n",
    "train_file = case_name + '_e2c_train' + case_suffix + train_suffix + '_n%d_dt%dday_nt%d_nrun%d.mat' %(n_train_step, dt, num_t, n_train_run)\n",
    "eval_file = case_name + '_e2c_eval' + case_suffix + train_suffix +'_n%d_dt%dday_nt%d_nrun%d.mat' %(n_eval_step, dt, num_t, n_eval_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_t_eval.shape:  (2000, 60, 60, 2)\n",
      "state_t1_eval.shape:  (2000, 60, 60, 2)\n",
      "bhp_eval.shape:  (2000, 18)\n",
      "dt_eval.shape:  (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "#################### model specification ##################\n",
    "epoch = 10\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "latent_dim = 50\n",
    "\n",
    "u_dim = 9*2 # control dimension, gaussian 9 wells\n",
    "\n",
    "# load data\n",
    "hf_r = h5py.File(data_dir + train_file, 'r')\n",
    "state_t_train = np.array(hf_r.get('state_t'))\n",
    "state_t1_train = np.array(hf_r.get('state_t1'))\n",
    "bhp_train = np.array(hf_r.get('bhp'))\n",
    "dt_train = np.array(hf_r.get('dt'))\n",
    "hf_r.close()\n",
    "\n",
    "num_train = state_t_train.shape[0]\n",
    "# dt_train = np.ones((num_train,1)) # dt=20days, normalized to 1\n",
    "\n",
    "hf_r = h5py.File(data_dir + eval_file, 'r')\n",
    "state_t_eval = np.array(hf_r.get('state_t'))\n",
    "state_t1_eval = np.array(hf_r.get('state_t1'))\n",
    "bhp_eval = np.array(hf_r.get('bhp'))\n",
    "dt_eval = np.array(hf_r.get('dt'))\n",
    "hf_r.close()\n",
    "\n",
    "print(\"state_t_eval.shape: \", state_t_eval.shape)\n",
    "print(\"state_t1_eval.shape: \", state_t1_eval.shape)\n",
    "print(\"bhp_eval.shape: \", bhp_eval.shape)\n",
    "print(\"dt_eval.shape: \", dt_eval.shape)\n",
    "\n",
    "\n",
    "num_eval = state_t_eval.shape[0]\n",
    "# dt_eval = np.ones((num_eval, 1)) # dt=20days, normalized to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load permeability data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m shape is  (60, 60, 1)\n",
      "m_eval shape is  (2000, 60, 60, 1)\n",
      "m shape is  (6000, 60, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "m = np.loadtxt(\"/data3/Astro/personal/zjin/sim_runs/case8_9w_bhp_rate_ms_gau/template/logk1.dat\") # Gaussian\n",
    "\n",
    "m = m.reshape(60, 60, 1)\n",
    "print('m shape is ', m.shape)\n",
    "\n",
    "m_tf = Input(shape=(60, 60, 1))\n",
    "\n",
    "m_eval = np.repeat(np.expand_dims(m, axis = 0), state_t_eval.shape[0], axis = 0)\n",
    "print(\"m_eval shape is \", m_eval.shape)\n",
    "\n",
    "m = np.repeat(np.expand_dims(m,axis = 0), state_t_train.shape[0], axis = 0)\n",
    "print(\"m shape is \", m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load well location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5\n",
      "prod_loc:\n",
      "[[10 10]\n",
      " [ 8 52]\n",
      " [30 30]\n",
      " [50 10]\n",
      " [50 50]]\n",
      "(5, 2)\n",
      "prod_loc shape is  (5, 2)\n"
     ]
    }
   ],
   "source": [
    "well_loc_file = '/data3/Astro/personal/zjin/sim_runs/case8_9w_bhp_rate_ms_gau/template/well_loc00.dat'\n",
    "\n",
    "well_loc = np.loadtxt(well_loc_file).astype(int)\n",
    "num_prod = well_loc[0,0]\n",
    "num_inj = well_loc[0,1]\n",
    "num_well = num_prod+num_inj\n",
    "print(num_inj, num_prod)\n",
    "\n",
    "prod_loc = well_loc[1:num_prod+1,:]\n",
    "print(\"prod_loc:\\n{}\".format(prod_loc))\n",
    "print(prod_loc.shape)\n",
    "\n",
    "print('prod_loc shape is ', prod_loc.shape)\n",
    "prod_loc_tf = tf.placeholder(tf.int32, shape=(num_prod,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Construct E2C model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma =  0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (60, 60, 2)\n",
    "\n",
    "#############################################\n",
    "# Note\n",
    "# For E2C with AE, sigma = 0.0\n",
    "# For E2C with UAE framework, sigma != 0.0, (e.g., =0.001)\n",
    "#############################################\n",
    "encoder, decoder, transition = create_e2c(latent_dim, u_dim, input_shape, sigma=0.0) \n",
    "\n",
    "\n",
    "xt = Input(shape=input_shape)\n",
    "xt1 = Input(shape=input_shape)\n",
    "ut = Input(shape=(u_dim, ))\n",
    "dt = Input(shape=(1,))\n",
    "\n",
    "zt = encoder(xt)\n",
    "xt_rec = decoder(zt)\n",
    "\n",
    "zt1 = encoder(xt1)\n",
    "\n",
    "zt1_pred = transition([zt, ut, dt])\n",
    "xt1_pred = decoder(zt1_pred)\n",
    "\n",
    "# Compute loss\n",
    "loss_rec_t = reconstruction_loss(xt, xt_rec)\n",
    "loss_rec_t1 = reconstruction_loss(xt1, xt1_pred)\n",
    "\n",
    "loss_flux_t = get_flux_loss(m_tf, xt, xt_rec) / 1000.\n",
    "loss_flux_t1 = get_flux_loss(m_tf, xt1, xt1_pred) / 1000.\n",
    "\n",
    "binary_sat_loss_t = get_binary_sat_loss(xt, xt_rec) * 1\n",
    "binary_sat_loss_t1 = get_binary_sat_loss(xt1, xt1_pred) * 1\n",
    "\n",
    "loss_prod_bhp_t = get_well_bhp_loss(xt, xt_rec, prod_loc_tf) * 20\n",
    "loss_prod_bhp_t1 = get_well_bhp_loss(xt1, xt1_pred, prod_loc_tf) * 20\n",
    "\n",
    "loss_l2_reg = l2_reg_loss(zt)  # log(1.) = 0.\n",
    "\n",
    "loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg  + loss_flux_t + loss_flux_t1 + loss_prod_bhp_t + loss_prod_bhp_t1 # JPSE 2020 Gaussian case\n",
    "\n",
    "#####################################################################\n",
    "# Note: you can also use other combination to construct loss function\n",
    "# loss_bound = loss_rec_t + loss_rec_t1  + loss_flux_t + loss_flux_t1 + loss_prod_bhp_t + loss_prod_bhp_t1 # UAE\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg # no flux/bhp loss comparison\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg  + loss_flux_t + loss_flux_t1\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_kl + binary_sat_loss_t + binary_sat_loss_t1\n",
    "#####################################################################\n",
    "\n",
    "# Use zt_logvar to approximate zt1_logvar_pred\n",
    "loss_trans = l2_reg_loss(zt1_pred - zt1)\n",
    "# loss_trans = kl_normal_loss(zt1_mean_pred, zt1_logvar_pred, zt1_mean, zt1_logvar)\n",
    "\n",
    "\n",
    "trans_loss_weight = 1.0 # lambda in E2C paper Eq. (11)\n",
    "loss = loss_bound + trans_loss_weight * loss_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create log for `Tensorboard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-14-21:57\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def write_summary(value, tag, summary_writer, global_step):\n",
    "    \"\"\"Write a single summary value to tensorboard\"\"\"\n",
    "    summary = tf.Summary()\n",
    "    summary.value.add(tag=tag, simple_value=value)\n",
    "    summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "## used to generate log directory\n",
    "currentDT = datetime.now()\n",
    "current_time = str(currentDT).replace(\" \", \"-\")[:-10]\n",
    "print(current_time)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('logs/' + case_name + case_suffix + '_ep' + str(epoch) + '_tr' + str(n_train_run) + '_' + current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct computation graph (only necessary for `Tensorflow 1.x`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "opt = Adam(lr=learning_rate)\n",
    "\n",
    "trainable_weights = encoder.trainable_weights + decoder.trainable_weights + transition.trainable_weights\n",
    "\n",
    "updates = opt.get_updates(loss, trainable_weights)\n",
    "\n",
    "iterate = K.function([xt, ut, xt1, m_tf, prod_loc_tf, dt], [loss, loss_rec_t, loss_rec_t1, loss_l2_reg, loss_trans, loss_flux_t, loss_flux_t1, loss_prod_bhp_t, loss_prod_bhp_t1], updates=updates)\n",
    "\n",
    "eval_loss = K.function([xt, ut, xt1, m_tf, prod_loc_tf, dt], [loss])\n",
    "\n",
    "num_batch = int(num_train/batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 1/1500, Loss 16255.943359, Loss rec 7905.526367, loss rec t1 8109.102051, loss kl 0.095494, loss_trans 0.123232, loss flux 84.120415, loss flux t1 83.694725, prod bhp loss 36.492275, prod bhp loss t1 36.789158\n",
      "Epoch 1/10, Batch 51/1500, Loss 1160.412354, Loss rec 412.410828, loss rec t1 398.197998, loss kl 5.594963, loss_trans 6.637484, loss flux 155.295425, loss flux t1 158.564209, prod bhp loss 11.239604, prod bhp loss t1 12.471746\n",
      "Epoch 1/10, Batch 101/1500, Loss 668.391174, Loss rec 198.851837, loss rec t1 195.120255, loss kl 5.650772, loss_trans 5.989245, loss flux 125.026306, loss flux t1 120.819000, prod bhp loss 8.329857, prod bhp loss t1 8.603966\n",
      "Epoch 1/10, Batch 151/1500, Loss 466.853210, Loss rec 150.010742, loss rec t1 134.289261, loss kl 5.009523, loss_trans 4.554918, loss flux 81.652237, loss flux t1 77.879219, prod bhp loss 7.634868, prod bhp loss t1 5.822484\n",
      "Epoch 1/10, Batch 201/1500, Loss 428.377136, Loss rec 149.671326, loss rec t1 147.492157, loss kl 4.573234, loss_trans 3.803046, loss flux 56.985981, loss flux t1 56.622112, prod bhp loss 5.926626, prod bhp loss t1 3.302647\n",
      "Epoch 1/10, Batch 251/1500, Loss 440.809570, Loss rec 185.078293, loss rec t1 158.976440, loss kl 3.123225, loss_trans 2.621807, loss flux 43.299244, loss flux t1 40.470238, prod bhp loss 3.705266, prod bhp loss t1 3.535029\n",
      "Epoch 1/10, Batch 301/1500, Loss 399.438599, Loss rec 160.922836, loss rec t1 149.523651, loss kl 3.273181, loss_trans 2.047180, loss flux 38.492783, loss flux t1 37.135899, prod bhp loss 5.037688, prod bhp loss t1 3.005382\n",
      "Epoch 1/10, Batch 351/1500, Loss 329.426758, Loss rec 138.006653, loss rec t1 98.893677, loss kl 3.223527, loss_trans 2.356973, loss flux 39.266602, loss flux t1 39.872257, prod bhp loss 4.174042, prod bhp loss t1 3.633049\n",
      "Epoch 1/10, Batch 401/1500, Loss 359.780975, Loss rec 128.285858, loss rec t1 125.617065, loss kl 2.115524, loss_trans 1.543688, loss flux 48.299927, loss flux t1 41.704159, prod bhp loss 8.576681, prod bhp loss t1 3.638070\n",
      "Epoch 1/10, Batch 451/1500, Loss 377.431915, Loss rec 152.017319, loss rec t1 154.340378, loss kl 2.244342, loss_trans 1.285338, loss flux 28.949814, loss flux t1 30.943872, prod bhp loss 3.635198, prod bhp loss t1 4.015638\n",
      "Epoch 1/10, Batch 501/1500, Loss 218.813278, Loss rec 80.683533, loss rec t1 63.596237, loss kl 2.464363, loss_trans 1.334007, loss flux 33.244953, loss flux t1 31.137959, prod bhp loss 4.237241, prod bhp loss t1 2.114997\n",
      "Epoch 1/10, Batch 551/1500, Loss 204.737747, Loss rec 64.169960, loss rec t1 69.199799, loss kl 2.115457, loss_trans 0.952848, loss flux 33.257450, loss flux t1 30.282087, prod bhp loss 2.303037, prod bhp loss t1 2.457105\n",
      "Epoch 1/10, Batch 601/1500, Loss 237.095535, Loss rec 87.313385, loss rec t1 76.956902, loss kl 1.978702, loss_trans 0.946673, loss flux 33.112759, loss flux t1 31.730572, prod bhp loss 2.542079, prod bhp loss t1 2.514457\n",
      "Epoch 1/10, Batch 651/1500, Loss 175.402405, Loss rec 48.042084, loss rec t1 51.632286, loss kl 2.573916, loss_trans 0.917871, loss flux 36.564690, loss flux t1 29.966436, prod bhp loss 3.860440, prod bhp loss t1 1.844684\n",
      "Epoch 1/10, Batch 701/1500, Loss 125.744568, Loss rec 31.211285, loss rec t1 38.893539, loss kl 1.561148, loss_trans 0.633542, loss flux 26.018427, loss flux t1 23.035658, prod bhp loss 2.456181, prod bhp loss t1 1.934775\n",
      "Epoch 1/10, Batch 751/1500, Loss 117.860649, Loss rec 26.186192, loss rec t1 36.180870, loss kl 1.887401, loss_trans 0.588680, loss flux 25.699478, loss flux t1 22.996643, prod bhp loss 1.934335, prod bhp loss t1 2.387057\n",
      "Epoch 1/10, Batch 801/1500, Loss 148.606216, Loss rec 47.885742, loss rec t1 48.094330, loss kl 2.347428, loss_trans 0.692385, loss flux 22.673344, loss flux t1 22.898033, prod bhp loss 1.913728, prod bhp loss t1 2.101239\n",
      "Epoch 1/10, Batch 851/1500, Loss 94.656944, Loss rec 26.031509, loss rec t1 21.901211, loss kl 1.567517, loss_trans 0.397048, loss flux 20.976282, loss flux t1 20.328060, prod bhp loss 2.059736, prod bhp loss t1 1.395573\n",
      "Epoch 1/10, Batch 901/1500, Loss 102.272568, Loss rec 25.569960, loss rec t1 26.916576, loss kl 1.323661, loss_trans 0.327671, loss flux 23.250645, loss flux t1 21.793253, prod bhp loss 1.495602, prod bhp loss t1 1.595208\n",
      "Epoch 1/10, Batch 951/1500, Loss 93.564178, Loss rec 23.262131, loss rec t1 22.307632, loss kl 1.206541, loss_trans 0.324403, loss flux 23.515388, loss flux t1 18.048828, prod bhp loss 3.537270, prod bhp loss t1 1.361985\n",
      "Epoch 1/10, Batch 1001/1500, Loss 81.192406, Loss rec 19.673195, loss rec t1 21.437298, loss kl 1.568804, loss_trans 0.337335, loss flux 18.412228, loss flux t1 17.114531, prod bhp loss 1.307648, prod bhp loss t1 1.341373\n",
      "Epoch 1/10, Batch 1051/1500, Loss 98.821198, Loss rec 26.082531, loss rec t1 30.051071, loss kl 1.411317, loss_trans 0.309384, loss flux 19.329454, loss flux t1 18.662704, prod bhp loss 1.600760, prod bhp loss t1 1.373969\n",
      "Epoch 1/10, Batch 1101/1500, Loss 131.044357, Loss rec 48.592216, loss rec t1 36.104416, loss kl 1.796187, loss_trans 0.317692, loss flux 19.925224, loss flux t1 19.406641, prod bhp loss 2.767691, prod bhp loss t1 2.134291\n",
      "Epoch 1/10, Batch 1151/1500, Loss 85.165329, Loss rec 19.011490, loss rec t1 25.409023, loss kl 1.394616, loss_trans 0.264448, loss flux 18.300892, loss flux t1 18.122202, prod bhp loss 1.201786, prod bhp loss t1 1.460874\n",
      "Epoch 1/10, Batch 1201/1500, Loss 194.254669, Loss rec 83.251282, loss rec t1 54.411602, loss kl 1.368761, loss_trans 0.205070, loss flux 23.800465, loss flux t1 21.853884, prod bhp loss 5.229137, prod bhp loss t1 4.134473\n",
      "Epoch 1/10, Batch 1251/1500, Loss 91.311989, Loss rec 18.544273, loss rec t1 27.555780, loss kl 0.852107, loss_trans 0.165492, loss flux 18.710049, loss flux t1 21.380110, prod bhp loss 1.648784, prod bhp loss t1 2.455402\n",
      "Epoch 1/10, Batch 1301/1500, Loss 87.409660, Loss rec 24.694241, loss rec t1 21.035217, loss kl 1.616837, loss_trans 0.320646, loss flux 18.276194, loss flux t1 18.012676, prod bhp loss 1.860474, prod bhp loss t1 1.593374\n",
      "Epoch 1/10, Batch 1351/1500, Loss 68.128181, Loss rec 14.171144, loss rec t1 18.890083, loss kl 1.729403, loss_trans 0.232962, loss flux 15.258763, loss flux t1 15.131021, prod bhp loss 0.905370, prod bhp loss t1 1.809435\n",
      "Epoch 1/10, Batch 1401/1500, Loss 62.969124, Loss rec 15.289149, loss rec t1 15.906729, loss kl 1.424833, loss_trans 0.216152, loss flux 13.692090, loss flux t1 13.849988, prod bhp loss 1.267059, prod bhp loss t1 1.323125\n",
      "Epoch 1/10, Batch 1451/1500, Loss 72.592812, Loss rec 17.241680, loss rec t1 19.476437, loss kl 1.184601, loss_trans 0.177099, loss flux 16.243467, loss flux t1 15.689929, prod bhp loss 1.063746, prod bhp loss t1 1.515857\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 1/10, Train loss 59.091694, Eval loss 64.437859\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 2/10, Batch 1/1500, Loss 64.007034, Loss rec 12.460291, loss rec t1 15.146046, loss kl 1.034854, loss_trans 0.071678, loss flux 16.857948, loss flux t1 15.650791, prod bhp loss 1.252143, prod bhp loss t1 1.533288\n",
      "Epoch 2/10, Batch 51/1500, Loss 68.273941, Loss rec 17.124348, loss rec t1 17.259121, loss kl 1.458182, loss_trans 0.160177, loss flux 14.653746, loss flux t1 14.894829, prod bhp loss 1.498284, prod bhp loss t1 1.225248\n",
      "Epoch 2/10, Batch 101/1500, Loss 63.297836, Loss rec 13.470653, loss rec t1 17.361490, loss kl 1.044392, loss_trans 0.131931, loss flux 14.127460, loss flux t1 14.340607, prod bhp loss 1.390373, prod bhp loss t1 1.430931\n",
      "Epoch 2/10, Batch 151/1500, Loss 51.421799, Loss rec 8.506542, loss rec t1 12.962889, loss kl 0.855184, loss_trans 0.096030, loss flux 13.170701, loss flux t1 13.656358, prod bhp loss 1.031788, prod bhp loss t1 1.142306\n",
      "Epoch 2/10, Batch 201/1500, Loss 53.622025, Loss rec 9.492294, loss rec t1 12.033989, loss kl 0.970508, loss_trans 0.071354, loss flux 13.444075, loss flux t1 15.288861, prod bhp loss 1.080172, prod bhp loss t1 1.240774\n",
      "Epoch 2/10, Batch 251/1500, Loss 79.684166, Loss rec 21.399473, loss rec t1 20.953484, loss kl 0.826645, loss_trans 0.098664, loss flux 17.856478, loss flux t1 16.206978, prod bhp loss 1.311329, prod bhp loss t1 1.031123\n",
      "Epoch 2/10, Batch 301/1500, Loss 60.415585, Loss rec 13.894104, loss rec t1 15.392563, loss kl 0.944282, loss_trans 0.070953, loss flux 14.315208, loss flux t1 13.250494, prod bhp loss 1.514925, prod bhp loss t1 1.033052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Batch 351/1500, Loss 55.821453, Loss rec 12.571054, loss rec t1 13.432755, loss kl 1.035141, loss_trans 0.101500, loss flux 12.977447, loss flux t1 13.115450, prod bhp loss 1.357528, prod bhp loss t1 1.230571\n",
      "Epoch 2/10, Batch 401/1500, Loss 56.261532, Loss rec 6.859416, loss rec t1 13.218897, loss kl 0.880338, loss_trans 0.067022, loss flux 14.539680, loss flux t1 17.662218, prod bhp loss 1.463868, prod bhp loss t1 1.570096\n",
      "Epoch 2/10, Batch 451/1500, Loss 74.326057, Loss rec 25.178219, loss rec t1 16.811947, loss kl 0.978809, loss_trans 0.095291, loss flux 14.023904, loss flux t1 13.589661, prod bhp loss 1.788421, prod bhp loss t1 1.859811\n",
      "Epoch 2/10, Batch 501/1500, Loss 67.391785, Loss rec 15.725797, loss rec t1 21.038464, loss kl 1.010575, loss_trans 0.081906, loss flux 13.407604, loss flux t1 13.290692, prod bhp loss 1.156689, prod bhp loss t1 1.680052\n",
      "Epoch 2/10, Batch 551/1500, Loss 51.407887, Loss rec 9.695971, loss rec t1 11.801929, loss kl 0.868841, loss_trans 0.048704, loss flux 13.329542, loss flux t1 12.636915, prod bhp loss 1.209972, prod bhp loss t1 1.816013\n",
      "Epoch 2/10, Batch 601/1500, Loss 68.539825, Loss rec 14.504830, loss rec t1 18.568886, loss kl 0.934654, loss_trans 0.053887, loss flux 14.485210, loss flux t1 14.844234, prod bhp loss 2.416484, prod bhp loss t1 2.731646\n",
      "Epoch 2/10, Batch 651/1500, Loss 51.670216, Loss rec 8.910599, loss rec t1 12.102488, loss kl 1.241715, loss_trans 0.065801, loss flux 12.708122, loss flux t1 14.140065, prod bhp loss 1.144235, prod bhp loss t1 1.357196\n",
      "Epoch 2/10, Batch 701/1500, Loss 49.687263, Loss rec 9.927835, loss rec t1 11.119267, loss kl 0.693952, loss_trans 0.049268, loss flux 13.038115, loss flux t1 11.630686, prod bhp loss 1.968270, prod bhp loss t1 1.259876\n",
      "Epoch 2/10, Batch 751/1500, Loss 49.150269, Loss rec 10.359823, loss rec t1 9.437684, loss kl 0.910138, loss_trans 0.050790, loss flux 13.029368, loss flux t1 12.051216, prod bhp loss 1.636129, prod bhp loss t1 1.675120\n",
      "Epoch 2/10, Batch 801/1500, Loss 58.926884, Loss rec 13.056262, loss rec t1 15.322654, loss kl 1.203808, loss_trans 0.053385, loss flux 13.600143, loss flux t1 13.475013, prod bhp loss 0.958537, prod bhp loss t1 1.257075\n",
      "Epoch 2/10, Batch 851/1500, Loss 43.982590, Loss rec 8.173927, loss rec t1 8.753617, loss kl 0.761655, loss_trans 0.023612, loss flux 12.151965, loss flux t1 11.933848, prod bhp loss 1.053574, prod bhp loss t1 1.130390\n",
      "Epoch 2/10, Batch 901/1500, Loss 45.608807, Loss rec 8.264088, loss rec t1 9.418390, loss kl 0.682278, loss_trans 0.033678, loss flux 12.540604, loss flux t1 12.162295, prod bhp loss 1.208710, prod bhp loss t1 1.298760\n",
      "Epoch 2/10, Batch 951/1500, Loss 49.293716, Loss rec 11.287381, loss rec t1 10.984478, loss kl 0.608661, loss_trans 0.028821, loss flux 12.612029, loss flux t1 10.848704, prod bhp loss 1.619252, prod bhp loss t1 1.304388\n",
      "Epoch 2/10, Batch 1001/1500, Loss 43.769333, Loss rec 9.191893, loss rec t1 9.516928, loss kl 0.790863, loss_trans 0.026731, loss flux 11.502537, loss flux t1 10.320084, prod bhp loss 1.173798, prod bhp loss t1 1.246502\n",
      "Epoch 2/10, Batch 1051/1500, Loss 47.246914, Loss rec 9.150740, loss rec t1 11.507798, loss kl 0.767357, loss_trans 0.028769, loss flux 11.518433, loss flux t1 11.216534, prod bhp loss 1.708024, prod bhp loss t1 1.349260\n",
      "Epoch 2/10, Batch 1101/1500, Loss 92.796135, Loss rec 24.431440, loss rec t1 33.175682, loss kl 1.056706, loss_trans 0.029299, loss flux 14.988124, loss flux t1 14.529530, prod bhp loss 1.932690, prod bhp loss t1 2.652674\n",
      "Epoch 2/10, Batch 1151/1500, Loss 54.519882, Loss rec 12.858582, loss rec t1 11.995310, loss kl 0.807847, loss_trans 0.037140, loss flux 13.507447, loss flux t1 12.903821, prod bhp loss 1.206805, prod bhp loss t1 1.202927\n",
      "Epoch 2/10, Batch 1201/1500, Loss 53.917675, Loss rec 12.047988, loss rec t1 13.455561, loss kl 0.710145, loss_trans 0.029575, loss flux 12.899991, loss flux t1 12.070086, prod bhp loss 1.051479, prod bhp loss t1 1.652853\n",
      "Epoch 2/10, Batch 1251/1500, Loss 108.160103, Loss rec 27.158682, loss rec t1 33.847816, loss kl 0.483482, loss_trans 0.032970, loss flux 20.621078, loss flux t1 19.193220, prod bhp loss 3.626318, prod bhp loss t1 3.196533\n",
      "Epoch 2/10, Batch 1301/1500, Loss 58.831242, Loss rec 13.158716, loss rec t1 13.402411, loss kl 0.950082, loss_trans 0.048560, loss flux 14.221655, loss flux t1 14.123137, prod bhp loss 1.681194, prod bhp loss t1 1.245482\n",
      "Epoch 2/10, Batch 1351/1500, Loss 42.542812, Loss rec 8.494339, loss rec t1 8.721592, loss kl 1.033328, loss_trans 0.032842, loss flux 10.920095, loss flux t1 11.006755, prod bhp loss 1.254256, prod bhp loss t1 1.079605\n",
      "Epoch 2/10, Batch 1401/1500, Loss 40.280739, Loss rec 8.398208, loss rec t1 8.786755, loss kl 0.783274, loss_trans 0.031985, loss flux 10.111970, loss flux t1 9.931623, prod bhp loss 1.067256, prod bhp loss t1 1.169661\n",
      "Epoch 2/10, Batch 1451/1500, Loss 46.297901, Loss rec 9.989132, loss rec t1 10.302156, loss kl 0.708433, loss_trans 0.036263, loss flux 11.780448, loss flux t1 10.970872, prod bhp loss 1.120253, prod bhp loss t1 1.390343\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 2/10, Train loss 36.291492, Eval loss 40.212566\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 3/10, Batch 1/1500, Loss 42.577801, Loss rec 7.074986, loss rec t1 10.265154, loss kl 0.669876, loss_trans 0.020558, loss flux 11.443052, loss flux t1 10.750072, prod bhp loss 0.944902, prod bhp loss t1 1.409201\n",
      "Epoch 3/10, Batch 51/1500, Loss 45.110226, Loss rec 10.090448, loss rec t1 10.417134, loss kl 0.872081, loss_trans 0.030556, loss flux 10.819310, loss flux t1 10.578300, prod bhp loss 1.346756, prod bhp loss t1 0.955640\n",
      "Epoch 3/10, Batch 101/1500, Loss 38.582123, Loss rec 7.346026, loss rec t1 8.171602, loss kl 0.631628, loss_trans 0.023766, loss flux 10.141774, loss flux t1 9.992739, prod bhp loss 1.311961, prod bhp loss t1 0.962627\n",
      "Epoch 3/10, Batch 151/1500, Loss 34.846432, Loss rec 5.567736, loss rec t1 7.313135, loss kl 0.540666, loss_trans 0.022565, loss flux 9.237363, loss flux t1 10.204805, prod bhp loss 0.901946, prod bhp loss t1 1.058217\n",
      "Epoch 3/10, Batch 201/1500, Loss 39.750099, Loss rec 7.356616, loss rec t1 8.480076, loss kl 0.631575, loss_trans 0.014457, loss flux 9.604952, loss flux t1 11.141226, prod bhp loss 1.304502, prod bhp loss t1 1.216698\n",
      "Epoch 3/10, Batch 251/1500, Loss 41.321598, Loss rec 7.894058, loss rec t1 9.052295, loss kl 0.543159, loss_trans 0.019955, loss flux 11.093336, loss flux t1 10.591029, prod bhp loss 1.203159, prod bhp loss t1 0.924605\n",
      "Epoch 3/10, Batch 301/1500, Loss 40.632099, Loss rec 8.312164, loss rec t1 9.275696, loss kl 0.615255, loss_trans 0.013040, loss flux 10.531507, loss flux t1 9.699517, prod bhp loss 1.365623, prod bhp loss t1 0.819296\n",
      "Epoch 3/10, Batch 351/1500, Loss 40.392284, Loss rec 8.830402, loss rec t1 8.851501, loss kl 0.629272, loss_trans 0.020079, loss flux 9.796773, loss flux t1 9.985262, prod bhp loss 1.396956, prod bhp loss t1 0.882039\n",
      "Epoch 3/10, Batch 401/1500, Loss 36.847439, Loss rec 4.345039, loss rec t1 7.851830, loss kl 0.643732, loss_trans 0.025571, loss flux 9.462273, loss flux t1 12.336231, prod bhp loss 0.987803, prod bhp loss t1 1.194959\n",
      "Epoch 3/10, Batch 451/1500, Loss 37.991341, Loss rec 7.809131, loss rec t1 9.508041, loss kl 0.551332, loss_trans 0.025822, loss flux 9.056125, loss flux t1 8.958895, prod bhp loss 0.922806, prod bhp loss t1 1.159193\n",
      "Epoch 3/10, Batch 501/1500, Loss 38.865726, Loss rec 7.110222, loss rec t1 9.587651, loss kl 0.667632, loss_trans 0.019863, loss flux 9.890261, loss flux t1 9.714079, prod bhp loss 0.778212, prod bhp loss t1 1.097806\n",
      "Epoch 3/10, Batch 551/1500, Loss 42.965996, Loss rec 9.629655, loss rec t1 8.273876, loss kl 0.603895, loss_trans 0.016024, loss flux 10.974274, loss flux t1 10.850952, prod bhp loss 1.387368, prod bhp loss t1 1.229952\n",
      "Epoch 3/10, Batch 601/1500, Loss 44.228180, Loss rec 9.036506, loss rec t1 8.505252, loss kl 0.654397, loss_trans 0.019011, loss flux 12.236104, loss flux t1 11.239837, prod bhp loss 1.280565, prod bhp loss t1 1.256510\n",
      "Epoch 3/10, Batch 651/1500, Loss 38.166134, Loss rec 6.485099, loss rec t1 7.632573, loss kl 0.860902, loss_trans 0.019056, loss flux 10.033243, loss flux t1 11.073879, prod bhp loss 0.813837, prod bhp loss t1 1.247542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Batch 701/1500, Loss 35.808552, Loss rec 6.576118, loss rec t1 7.164904, loss kl 0.466675, loss_trans 0.017300, loss flux 10.199799, loss flux t1 9.044996, prod bhp loss 1.292563, prod bhp loss t1 1.046195\n",
      "Epoch 3/10, Batch 751/1500, Loss 43.228092, Loss rec 10.820006, loss rec t1 6.371902, loss kl 0.652305, loss_trans 0.015302, loss flux 11.560196, loss flux t1 10.510592, prod bhp loss 1.960477, prod bhp loss t1 1.337314\n",
      "Epoch 3/10, Batch 801/1500, Loss 40.089634, Loss rec 8.830076, loss rec t1 8.970471, loss kl 0.840605, loss_trans 0.016302, loss flux 9.670050, loss flux t1 9.491739, prod bhp loss 1.027796, prod bhp loss t1 1.242592\n",
      "Epoch 3/10, Batch 851/1500, Loss 34.633698, Loss rec 5.645653, loss rec t1 6.124381, loss kl 0.538767, loss_trans 0.008449, loss flux 10.294744, loss flux t1 9.962868, prod bhp loss 1.048960, prod bhp loss t1 1.009872\n",
      "Epoch 3/10, Batch 901/1500, Loss 34.383274, Loss rec 5.884270, loss rec t1 6.332466, loss kl 0.476199, loss_trans 0.018629, loss flux 9.783337, loss flux t1 9.429370, prod bhp loss 1.199902, prod bhp loss t1 1.259103\n",
      "Epoch 3/10, Batch 951/1500, Loss 92.404915, Loss rec 36.931374, loss rec t1 26.306107, loss kl 0.370339, loss_trans 0.013846, loss flux 13.561301, loss flux t1 11.941035, prod bhp loss 2.073597, prod bhp loss t1 1.207321\n",
      "Epoch 3/10, Batch 1001/1500, Loss 36.635544, Loss rec 7.915191, loss rec t1 7.358544, loss kl 0.596046, loss_trans 0.011568, loss flux 9.806038, loss flux t1 8.709219, prod bhp loss 1.224046, prod bhp loss t1 1.014892\n",
      "Epoch 3/10, Batch 1051/1500, Loss 37.133606, Loss rec 7.009996, loss rec t1 7.979892, loss kl 0.535755, loss_trans 0.012229, loss flux 9.671005, loss flux t1 9.262831, prod bhp loss 1.517395, prod bhp loss t1 1.144500\n",
      "Epoch 3/10, Batch 1101/1500, Loss 101.141624, Loss rec 35.820412, loss rec t1 32.827232, loss kl 0.821960, loss_trans 0.017908, loss flux 13.746210, loss flux t1 12.787274, prod bhp loss 2.613969, prod bhp loss t1 2.506657\n",
      "Epoch 3/10, Batch 1151/1500, Loss 65.799072, Loss rec 19.045349, loss rec t1 20.308395, loss kl 0.563969, loss_trans 0.014923, loss flux 11.697962, loss flux t1 11.178771, prod bhp loss 1.273374, prod bhp loss t1 1.716325\n",
      "Epoch 3/10, Batch 1201/1500, Loss 50.646488, Loss rec 12.977401, loss rec t1 11.527737, loss kl 0.580049, loss_trans 0.016388, loss flux 11.221243, loss flux t1 10.450350, prod bhp loss 1.965162, prod bhp loss t1 1.908159\n",
      "Epoch 3/10, Batch 1251/1500, Loss 37.273613, Loss rec 5.619183, loss rec t1 8.536894, loss kl 0.399644, loss_trans 0.017385, loss flux 9.088637, loss flux t1 11.191557, prod bhp loss 1.147995, prod bhp loss t1 1.272321\n",
      "Epoch 3/10, Batch 1301/1500, Loss 36.724857, Loss rec 6.532733, loss rec t1 6.801078, loss kl 0.730567, loss_trans 0.019377, loss flux 10.359208, loss flux t1 10.037360, prod bhp loss 1.170306, prod bhp loss t1 1.074223\n",
      "Epoch 3/10, Batch 1351/1500, Loss 33.945274, Loss rec 6.165321, loss rec t1 6.527037, loss kl 0.794855, loss_trans 0.012161, loss flux 9.178606, loss flux t1 9.128181, prod bhp loss 1.236048, prod bhp loss t1 0.903066\n",
      "Epoch 3/10, Batch 1401/1500, Loss 30.740103, Loss rec 5.684217, loss rec t1 6.136397, loss kl 0.589255, loss_trans 0.013225, loss flux 8.346515, loss flux t1 8.052478, prod bhp loss 0.890402, prod bhp loss t1 1.027615\n",
      "Epoch 3/10, Batch 1451/1500, Loss 37.707150, Loss rec 7.676723, loss rec t1 7.839003, loss kl 0.548929, loss_trans 0.019780, loss flux 9.941756, loss flux t1 9.371837, prod bhp loss 1.101325, prod bhp loss t1 1.207794\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 3/10, Train loss 37.811630, Eval loss 36.602905\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 4/10, Batch 1/1500, Loss 36.099236, Loss rec 5.802489, loss rec t1 6.748684, loss kl 0.524312, loss_trans 0.013642, loss flux 10.583800, loss flux t1 10.119798, prod bhp loss 1.029409, prod bhp loss t1 1.277104\n",
      "Epoch 4/10, Batch 51/1500, Loss 35.797215, Loss rec 7.543744, loss rec t1 7.374610, loss kl 0.659831, loss_trans 0.013833, loss flux 9.186478, loss flux t1 9.006425, prod bhp loss 1.290908, prod bhp loss t1 0.721387\n",
      "Epoch 4/10, Batch 101/1500, Loss 29.792332, Loss rec 5.262047, loss rec t1 5.543205, loss kl 0.473386, loss_trans 0.010064, loss flux 8.379501, loss flux t1 8.032775, prod bhp loss 1.199792, prod bhp loss t1 0.891563\n",
      "Epoch 4/10, Batch 151/1500, Loss 32.649357, Loss rec 5.526972, loss rec t1 7.025148, loss kl 0.409689, loss_trans 0.011880, loss flux 8.326251, loss flux t1 9.233145, prod bhp loss 0.999721, prod bhp loss t1 1.116553\n",
      "Epoch 4/10, Batch 201/1500, Loss 29.975849, Loss rec 4.035015, loss rec t1 5.811664, loss kl 0.479696, loss_trans 0.008513, loss flux 8.310065, loss flux t1 9.503248, prod bhp loss 0.832890, prod bhp loss t1 0.994760\n",
      "Epoch 4/10, Batch 251/1500, Loss 36.713135, Loss rec 6.644607, loss rec t1 7.811675, loss kl 0.418326, loss_trans 0.010726, loss flux 10.328903, loss flux t1 9.298116, prod bhp loss 1.093117, prod bhp loss t1 1.107668\n",
      "Epoch 4/10, Batch 301/1500, Loss 34.378021, Loss rec 6.238912, loss rec t1 7.517062, loss kl 0.466898, loss_trans 0.006896, loss flux 9.473004, loss flux t1 8.677632, prod bhp loss 1.300490, prod bhp loss t1 0.697126\n",
      "Epoch 4/10, Batch 351/1500, Loss 32.966988, Loss rec 5.828446, loss rec t1 7.817721, loss kl 0.482296, loss_trans 0.010984, loss flux 8.277737, loss flux t1 8.461215, prod bhp loss 1.154963, prod bhp loss t1 0.933624\n",
      "Epoch 4/10, Batch 401/1500, Loss 34.285381, Loss rec 3.995762, loss rec t1 5.675102, loss kl 0.517254, loss_trans 0.016570, loss flux 10.325691, loss flux t1 11.247834, prod bhp loss 1.271277, prod bhp loss t1 1.235891\n",
      "Epoch 4/10, Batch 451/1500, Loss 33.247616, Loss rec 7.396464, loss rec t1 7.154006, loss kl 0.411446, loss_trans 0.017948, loss flux 8.194058, loss flux t1 8.050738, prod bhp loss 0.972166, prod bhp loss t1 1.050786\n",
      "Epoch 4/10, Batch 501/1500, Loss 33.388451, Loss rec 6.022816, loss rec t1 8.131488, loss kl 0.506054, loss_trans 0.010050, loss flux 8.481607, loss flux t1 8.314080, prod bhp loss 0.844159, prod bhp loss t1 1.078199\n",
      "Epoch 4/10, Batch 551/1500, Loss 33.048973, Loss rec 6.393196, loss rec t1 5.804286, loss kl 0.480873, loss_trans 0.011620, loss flux 9.142045, loss flux t1 8.955825, prod bhp loss 1.171399, prod bhp loss t1 1.089728\n",
      "Epoch 4/10, Batch 601/1500, Loss 32.708450, Loss rec 5.679926, loss rec t1 5.727513, loss kl 0.518140, loss_trans 0.014855, loss flux 9.512239, loss flux t1 9.259588, prod bhp loss 1.090214, prod bhp loss t1 0.905977\n",
      "Epoch 4/10, Batch 651/1500, Loss 35.366074, Loss rec 6.585171, loss rec t1 8.722657, loss kl 0.677175, loss_trans 0.012342, loss flux 7.698854, loss flux t1 8.986279, prod bhp loss 1.250009, prod bhp loss t1 1.433588\n",
      "Epoch 4/10, Batch 701/1500, Loss 29.097290, Loss rec 4.933217, loss rec t1 5.015990, loss kl 0.357035, loss_trans 0.011837, loss flux 8.628180, loss flux t1 7.970471, prod bhp loss 1.213202, prod bhp loss t1 0.967359\n",
      "Epoch 4/10, Batch 751/1500, Loss 30.260220, Loss rec 4.696424, loss rec t1 5.250200, loss kl 0.496371, loss_trans 0.009205, loss flux 9.005917, loss flux t1 8.261044, prod bhp loss 1.353019, prod bhp loss t1 1.188041\n",
      "Epoch 4/10, Batch 801/1500, Loss 33.980824, Loss rec 6.489901, loss rec t1 7.208775, loss kl 0.663239, loss_trans 0.008394, loss flux 8.901661, loss flux t1 8.622357, prod bhp loss 0.955841, prod bhp loss t1 1.130657\n",
      "Epoch 4/10, Batch 851/1500, Loss 28.890730, Loss rec 4.258688, loss rec t1 4.508176, loss kl 0.423093, loss_trans 0.004755, loss flux 8.992708, loss flux t1 8.762166, prod bhp loss 1.143078, prod bhp loss t1 0.798066\n",
      "Epoch 4/10, Batch 901/1500, Loss 31.729570, Loss rec 5.591387, loss rec t1 5.615901, loss kl 0.370431, loss_trans 0.014947, loss flux 9.118815, loss flux t1 8.688412, prod bhp loss 1.040738, prod bhp loss t1 1.288940\n",
      "Epoch 4/10, Batch 951/1500, Loss 34.665562, Loss rec 6.344856, loss rec t1 6.294106, loss kl 0.346624, loss_trans 0.009516, loss flux 10.190003, loss flux t1 8.911304, prod bhp loss 1.710477, prod bhp loss t1 0.858676\n",
      "Epoch 4/10, Batch 1001/1500, Loss 29.973545, Loss rec 5.049544, loss rec t1 5.832277, loss kl 0.462527, loss_trans 0.006270, loss flux 8.777125, loss flux t1 7.857861, prod bhp loss 1.029194, prod bhp loss t1 0.958748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Batch 1051/1500, Loss 34.181419, Loss rec 6.720266, loss rec t1 6.977441, loss kl 0.426974, loss_trans 0.008253, loss flux 9.204556, loss flux t1 8.424800, prod bhp loss 1.468595, prod bhp loss t1 0.950530\n",
      "Epoch 4/10, Batch 1101/1500, Loss 64.736908, Loss rec 24.738918, loss rec t1 15.337611, loss kl 0.601292, loss_trans 0.010315, loss flux 10.664688, loss flux t1 9.584132, prod bhp loss 2.288093, prod bhp loss t1 1.511859\n",
      "Epoch 4/10, Batch 1151/1500, Loss 31.513262, Loss rec 5.973372, loss rec t1 6.060734, loss kl 0.458110, loss_trans 0.009047, loss flux 8.722811, loss flux t1 8.185197, prod bhp loss 1.062361, prod bhp loss t1 1.041627\n",
      "Epoch 4/10, Batch 1201/1500, Loss 35.690239, Loss rec 8.303267, loss rec t1 5.893958, loss kl 0.477225, loss_trans 0.013445, loss flux 9.522305, loss flux t1 8.732896, prod bhp loss 1.490510, prod bhp loss t1 1.256635\n",
      "Epoch 4/10, Batch 1251/1500, Loss 44.520538, Loss rec 7.800167, loss rec t1 13.423307, loss kl 0.326009, loss_trans 0.016610, loss flux 8.754464, loss flux t1 10.922335, prod bhp loss 1.742771, prod bhp loss t1 1.534878\n",
      "Epoch 4/10, Batch 1301/1500, Loss 40.027531, Loss rec 8.330750, loss rec t1 8.157553, loss kl 0.573788, loss_trans 0.014710, loss flux 10.587983, loss flux t1 10.002372, prod bhp loss 1.338762, prod bhp loss t1 1.021613\n",
      "Epoch 4/10, Batch 1351/1500, Loss 28.655937, Loss rec 4.979469, loss rec t1 5.415459, loss kl 0.655960, loss_trans 0.008124, loss flux 8.037206, loss flux t1 7.607768, prod bhp loss 1.161800, prod bhp loss t1 0.790150\n",
      "Epoch 4/10, Batch 1401/1500, Loss 26.280031, Loss rec 4.952147, loss rec t1 5.121837, loss kl 0.459858, loss_trans 0.009037, loss flux 7.091656, loss flux t1 6.896152, prod bhp loss 0.900287, prod bhp loss t1 0.849056\n",
      "Epoch 4/10, Batch 1451/1500, Loss 31.066662, Loss rec 5.810829, loss rec t1 5.854074, loss kl 0.444370, loss_trans 0.016722, loss flux 8.616225, loss flux t1 8.193008, prod bhp loss 1.102373, prod bhp loss t1 1.029061\n",
      "====================================================\n",
      "\n",
      "\n",
      "Epoch 4/10, Train loss 31.706656, Eval loss 31.193604\n",
      "\n",
      "\n",
      "====================================================\n",
      "Epoch 5/10, Batch 1/1500, Loss 32.952850, Loss rec 5.952793, loss rec t1 5.362002, loss kl 0.442424, loss_trans 0.011892, loss flux 9.738897, loss flux t1 9.178349, prod bhp loss 1.108203, prod bhp loss t1 1.158292\n",
      "Epoch 5/10, Batch 51/1500, Loss 28.248491, Loss rec 5.236836, loss rec t1 5.425404, loss kl 0.537628, loss_trans 0.008998, loss flux 7.720355, loss flux t1 7.428995, prod bhp loss 1.219827, prod bhp loss t1 0.670447\n",
      "Epoch 5/10, Batch 101/1500, Loss 25.070978, Loss rec 4.127619, loss rec t1 4.307878, loss kl 0.374444, loss_trans 0.006862, loss flux 7.400063, loss flux t1 7.013219, prod bhp loss 1.115750, prod bhp loss t1 0.725142\n",
      "Epoch 5/10, Batch 151/1500, Loss 27.001848, Loss rec 4.436887, loss rec t1 5.548656, loss kl 0.326316, loss_trans 0.009215, loss flux 6.931421, loss flux t1 7.891619, prod bhp loss 0.845327, prod bhp loss t1 1.012408\n",
      "Epoch 5/10, Batch 201/1500, Loss 24.964577, Loss rec 3.381664, loss rec t1 4.050336, loss kl 0.384190, loss_trans 0.006436, loss flux 7.217336, loss flux t1 8.106343, prod bhp loss 0.872342, prod bhp loss t1 0.945928\n",
      "Epoch 5/10, Batch 251/1500, Loss 37.371292, Loss rec 7.107128, loss rec t1 8.459113, loss kl 0.340762, loss_trans 0.007932, loss flux 10.012557, loss flux t1 9.036133, prod bhp loss 1.128604, prod bhp loss t1 1.279064\n",
      "Epoch 5/10, Batch 301/1500, Loss 30.329777, Loss rec 5.848892, loss rec t1 6.056075, loss kl 0.381361, loss_trans 0.005799, loss flux 8.508225, loss flux t1 7.530589, prod bhp loss 1.261593, prod bhp loss t1 0.737246\n",
      "Epoch 5/10, Batch 351/1500, Loss 32.897606, Loss rec 7.244743, loss rec t1 8.175272, loss kl 0.386895, loss_trans 0.007997, loss flux 7.613913, loss flux t1 7.752398, prod bhp loss 1.095348, prod bhp loss t1 0.621042\n",
      "Epoch 5/10, Batch 401/1500, Loss 29.234472, Loss rec 3.093066, loss rec t1 5.468988, loss kl 0.438175, loss_trans 0.013144, loss flux 8.419848, loss flux t1 9.620760, prod bhp loss 0.949974, prod bhp loss t1 1.230517\n",
      "Epoch 5/10, Batch 451/1500, Loss 29.067596, Loss rec 5.600810, loss rec t1 6.034383, loss kl 0.324403, loss_trans 0.018742, loss flux 7.829079, loss flux t1 7.359119, prod bhp loss 0.895176, prod bhp loss t1 1.005886\n",
      "Epoch 5/10, Batch 501/1500, Loss 33.834064, Loss rec 5.897298, loss rec t1 8.294082, loss kl 0.409692, loss_trans 0.007184, loss flux 8.787580, loss flux t1 8.683126, prod bhp loss 0.849557, prod bhp loss t1 0.905548\n",
      "Epoch 5/10, Batch 551/1500, Loss 28.078823, Loss rec 4.818839, loss rec t1 4.608584, loss kl 0.414524, loss_trans 0.011607, loss flux 8.267282, loss flux t1 7.959579, prod bhp loss 1.046308, prod bhp loss t1 0.952099\n",
      "Epoch 5/10, Batch 601/1500, Loss 30.206148, Loss rec 4.717263, loss rec t1 5.889853, loss kl 0.442029, loss_trans 0.015600, loss flux 8.473190, loss flux t1 8.292009, prod bhp loss 1.258169, prod bhp loss t1 1.118034\n",
      "Epoch 5/10, Batch 651/1500, Loss 25.772738, Loss rec 3.232398, loss rec t1 4.516726, loss kl 0.592920, loss_trans 0.013066, loss flux 7.313782, loss flux t1 8.217215, prod bhp loss 0.923761, prod bhp loss t1 0.962871\n",
      "Epoch 5/10, Batch 701/1500, Loss 25.494244, Loss rec 3.978627, loss rec t1 4.373446, loss kl 0.295658, loss_trans 0.011909, loss flux 7.867938, loss flux t1 7.111770, prod bhp loss 1.023520, prod bhp loss t1 0.831374\n",
      "Epoch 5/10, Batch 751/1500, Loss 27.746136, Loss rec 5.118382, loss rec t1 3.807188, loss kl 0.434070, loss_trans 0.009313, loss flux 8.643297, loss flux t1 7.481763, prod bhp loss 1.327020, prod bhp loss t1 0.925100\n",
      "Epoch 5/10, Batch 801/1500, Loss 30.718670, Loss rec 6.102393, loss rec t1 6.443307, loss kl 0.568687, loss_trans 0.007036, loss flux 7.941631, loss flux t1 7.507710, prod bhp loss 1.130944, prod bhp loss t1 1.016962\n",
      "Epoch 5/10, Batch 851/1500, Loss 25.434982, Loss rec 3.847652, loss rec t1 5.424088, loss kl 0.354519, loss_trans 0.003491, loss flux 7.149540, loss flux t1 6.645131, prod bhp loss 1.038019, prod bhp loss t1 0.972540\n",
      "Epoch 5/10, Batch 901/1500, Loss 24.281542, Loss rec 3.631543, loss rec t1 4.007913, loss kl 0.307670, loss_trans 0.012602, loss flux 7.412351, loss flux t1 6.824968, prod bhp loss 1.053554, prod bhp loss t1 1.030941\n",
      "Epoch 5/10, Batch 951/1500, Loss 42.260002, Loss rec 12.218025, loss rec t1 9.897656, loss kl 0.247740, loss_trans 0.007688, loss flux 9.297803, loss flux t1 8.328593, prod bhp loss 1.345485, prod bhp loss t1 0.917009\n",
      "Epoch 5/10, Batch 1001/1500, Loss 25.787540, Loss rec 5.147223, loss rec t1 4.542293, loss kl 0.402356, loss_trans 0.005295, loss flux 7.365299, loss flux t1 6.398029, prod bhp loss 1.129215, prod bhp loss t1 0.797831\n",
      "Epoch 5/10, Batch 1051/1500, Loss 26.095451, Loss rec 4.353151, loss rec t1 5.332773, loss kl 0.346133, loss_trans 0.004872, loss flux 7.017680, loss flux t1 6.671601, prod bhp loss 1.389894, prod bhp loss t1 0.979345\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    for ib in range(num_batch):\n",
    "        ind0 = ib * batch_size\n",
    "        state_t_batch  = state_t_train[ind0:ind0+batch_size, ...]\n",
    "        state_t1_batch = state_t1_train[ind0:ind0 + batch_size, ...]\n",
    "        bhp_batch      = bhp_train[ind0:ind0 + batch_size, ...]\n",
    "        m_batch        = m[ind0:ind0 + batch_size, ...]\n",
    "        dt_batch       = dt_train[ind0:ind0 + batch_size, ...]\n",
    "        \n",
    "        output = iterate([state_t_batch, bhp_batch, state_t1_batch, m_batch, prod_loc, dt_batch])\n",
    "        \n",
    "        n_itr = e * num_train + ib * batch_size + batch_size\n",
    "        write_summary(output[0], 'train/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[1]+output[2], 'train/sum_rec_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[5]+output[6], 'train/sum_flux_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[7]+output[8], 'train/sum_well_loss', summary_writer, n_itr) # log for tensorboard\n",
    "\n",
    "        if ib % 50 == 0:\n",
    "            print('Epoch %d/%d, Batch %d/%d, Loss %f, Loss rec %f, loss rec t1 %f, loss kl %f, loss_trans %f, loss flux %f, loss flux t1 %f, prod bhp loss %f, prod bhp loss t1 %f' % (e+1, epoch, ib+1, num_batch, output[0], output[1], output[2], output[3], output[4], output[5], output[6], output[7], output[8]))\n",
    "            eval_loss_val = eval_loss([state_t_eval, bhp_eval, state_t1_eval, m_eval, prod_loc, dt_eval])\n",
    "            write_summary(eval_loss_val[0], 'eval/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "    \n",
    "    print('====================================================')\n",
    "    print('\\n')\n",
    "    print('Epoch %d/%d, Train loss %f, Eval loss %f' % (e + 1, epoch, output[0], eval_loss_val[0]))\n",
    "    print('\\n')\n",
    "    print('====================================================')\n",
    "\n",
    "encoder.save_weights(output_dir + 'e2c_encoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' \\\n",
    "                     % (num_train, latent_dim, learning_rate, epoch))\n",
    "decoder.save_weights(output_dir + 'e2c_decoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' \\\n",
    "                     % (num_train, latent_dim, learning_rate, epoch))\n",
    "transition.save_weights(output_dir + 'e2c_transition_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' \\\n",
    "                        % (num_train, latent_dim, learning_rate, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.10.0]",
   "language": "python",
   "name": "conda-env-tf-1.10.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2C Training\n",
    "\n",
    "This is an interactive workflow for E2C training. Note that this specific case may generate results that are distinct from the paper.\n",
    "\n",
    "During the training process (while the last cell is running), you can monitor the training status with Tensorboard. Make sure `tensorboard` is installed properly. To install `tensorboard`:  \n",
    "`pip install tensorboard`  \n",
    "\n",
    "All the data used for `tensorboard` are stored in `logs/` directory. If you do not have `logs/` directory in your cloned repo, please create one. To turn on `tensorboard`:  \n",
    "`tensorboard --logdir=logs --port=5678` (`--port` is necesary for port-forwarding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Zhaoyang Larry Jin  \n",
    "Stanford University  \n",
    "zjin@stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load libaraies and config hardware (gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import e2c as e2c_util\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\", \"2\", \"3\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "# GPU memory management\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.session specification\n",
    "# TensorFlow wizardry\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Only allow a total of half the GPU memory to be allocated\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.75\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Specify params and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### case specification ######################\n",
    "\n",
    "data_dir = '../data/'\n",
    "output_dir = './saved_models/'\n",
    "\n",
    "case_name = '9w_ms_bhp_rate'\n",
    "case_suffix = '_fix_wl_rel_8'\n",
    "train_suffix = '_with_p'\n",
    "model_suffix = '_flux_loss'\n",
    "\n",
    "n_train_run = 300\n",
    "n_eval_run = 100\n",
    "num_t = 20 \n",
    "dt = 100\n",
    "n_train_step = n_train_run * num_t\n",
    "n_eval_step = n_eval_run * num_t\n",
    "\n",
    "\n",
    "train_file = case_name + '_e2c_train' + case_suffix + train_suffix + '_n%d_dt%dday_nt%d_nrun%d.mat' %(n_train_step, dt, num_t, n_train_run)\n",
    "eval_file = case_name + '_e2c_eval' + case_suffix + train_suffix +'_n%d_dt%dday_nt%d_nrun%d.mat' %(n_eval_step, dt, num_t, n_eval_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### model specification ##################\n",
    "epoch = 10\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4\n",
    "latent_dim = 50\n",
    "\n",
    "u_dim = 9*2 # control dimension, gaussian 9 wells\n",
    "\n",
    "# load data\n",
    "hf_r = h5py.File(data_dir + train_file, 'r')\n",
    "state_t_train = np.array(hf_r.get('state_t'))\n",
    "state_t1_train = np.array(hf_r.get('state_t1'))\n",
    "bhp_train = np.array(hf_r.get('bhp'))\n",
    "dt_train = np.array(hf_r.get('dt'))\n",
    "hf_r.close()\n",
    "\n",
    "num_train = state_t_train.shape[0]\n",
    "# dt_train = np.ones((num_train,1)) # dt=20days, normalized to 1\n",
    "\n",
    "hf_r = h5py.File(data_dir + eval_file, 'r')\n",
    "state_t_eval = np.array(hf_r.get('state_t'))\n",
    "state_t1_eval = np.array(hf_r.get('state_t1'))\n",
    "bhp_eval = np.array(hf_r.get('bhp'))\n",
    "dt_eval = np.array(hf_r.get('dt'))\n",
    "hf_r.close()\n",
    "\n",
    "print(\"state_t_eval.shape: \", state_t_eval.shape)\n",
    "print(\"state_t1_eval.shape: \", state_t1_eval.shape)\n",
    "print(\"bhp_eval.shape: \", bhp_eval.shape)\n",
    "print(\"dt_eval.shape: \", dt_eval.shape)\n",
    "\n",
    "\n",
    "num_eval = state_t_eval.shape[0]\n",
    "# dt_eval = np.ones((num_eval, 1)) # dt=20days, normalized to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load permeability data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.loadtxt(\"../data/template/logk1.dat\") # Gaussian\n",
    "\n",
    "m = m.reshape(60, 60, 1)\n",
    "print('m shape is ', m.shape)\n",
    "\n",
    "m_tf = Input(shape=(60, 60, 1))\n",
    "\n",
    "m_eval = np.repeat(np.expand_dims(m, axis = 0), state_t_eval.shape[0], axis = 0)\n",
    "print(\"m_eval shape is \", m_eval.shape)\n",
    "\n",
    "m = np.repeat(np.expand_dims(m,axis = 0), state_t_train.shape[0], axis = 0)\n",
    "print(\"m shape is \", m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load well location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_loc_file = '../data/template/well_loc00.dat'\n",
    "\n",
    "well_loc = np.loadtxt(well_loc_file).astype(int)\n",
    "num_prod = well_loc[0,0]\n",
    "num_inj = well_loc[0,1]\n",
    "num_well = num_prod+num_inj\n",
    "print(num_inj, num_prod)\n",
    "\n",
    "prod_loc = well_loc[1:num_prod+1,:]\n",
    "print(\"prod_loc:\\n{}\".format(prod_loc))\n",
    "print(prod_loc.shape)\n",
    "\n",
    "print('prod_loc shape is ', prod_loc.shape)\n",
    "prod_loc_tf = tf.placeholder(tf.int32, shape=(num_prod,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Construct E2C model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (60, 60, 2)\n",
    "\n",
    "#############################################\n",
    "# Note\n",
    "# For E2C with AE, sigma = 0.0\n",
    "# For E2C with UAE framework, sigma != 0.0, (e.g., =0.001)\n",
    "#############################################\n",
    "encoder, decoder, transition = create_e2c(latent_dim, u_dim, input_shape, sigma=0.0) \n",
    "\n",
    "\n",
    "xt = Input(shape=input_shape)\n",
    "xt1 = Input(shape=input_shape)\n",
    "ut = Input(shape=(u_dim, ))\n",
    "dt = Input(shape=(1,))\n",
    "\n",
    "zt = encoder(xt)\n",
    "xt_rec = decoder(zt)\n",
    "\n",
    "zt1 = encoder(xt1)\n",
    "\n",
    "zt1_pred = transition([zt, ut, dt])\n",
    "xt1_pred = decoder(zt1_pred)\n",
    "\n",
    "# Compute loss\n",
    "loss_rec_t = reconstruction_loss(xt, xt_rec)\n",
    "loss_rec_t1 = reconstruction_loss(xt1, xt1_pred)\n",
    "\n",
    "loss_flux_t = get_flux_loss(m_tf, xt, xt_rec) / 1000.\n",
    "loss_flux_t1 = get_flux_loss(m_tf, xt1, xt1_pred) / 1000.\n",
    "\n",
    "binary_sat_loss_t = get_binary_sat_loss(xt, xt_rec) * 1\n",
    "binary_sat_loss_t1 = get_binary_sat_loss(xt1, xt1_pred) * 1\n",
    "\n",
    "loss_prod_bhp_t = get_well_bhp_loss(xt, xt_rec, prod_loc_tf) * 20\n",
    "loss_prod_bhp_t1 = get_well_bhp_loss(xt1, xt1_pred, prod_loc_tf) * 20\n",
    "\n",
    "loss_l2_reg = l2_reg_loss(zt)  # log(1.) = 0.\n",
    "\n",
    "loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg  + loss_flux_t + loss_flux_t1 + loss_prod_bhp_t + loss_prod_bhp_t1 # JPSE 2020 Gaussian case\n",
    "\n",
    "#####################################################################\n",
    "# Note: you can also use other combination to construct loss function\n",
    "# loss_bound = loss_rec_t + loss_rec_t1  + loss_flux_t + loss_flux_t1 + loss_prod_bhp_t + loss_prod_bhp_t1 # UAE\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg # no flux/bhp loss comparison\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_l2_reg  + loss_flux_t + loss_flux_t1\n",
    "# loss_bound = loss_rec_t + loss_rec_t1 + loss_kl + binary_sat_loss_t + binary_sat_loss_t1\n",
    "#####################################################################\n",
    "\n",
    "# Use zt_logvar to approximate zt1_logvar_pred\n",
    "loss_trans = l2_reg_loss(zt1_pred - zt1)\n",
    "# loss_trans = kl_normal_loss(zt1_mean_pred, zt1_logvar_pred, zt1_mean, zt1_logvar)\n",
    "\n",
    "\n",
    "trans_loss_weight = 1.0 # lambda in E2C paper Eq. (11)\n",
    "loss = loss_bound + trans_loss_weight * loss_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create log for `Tensorboard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_summary(value, tag, summary_writer, global_step):\n",
    "    \"\"\"Write a single summary value to tensorboard\"\"\"\n",
    "    summary = tf.Summary()\n",
    "    summary.value.add(tag=tag, simple_value=value)\n",
    "    summary_writer.add_summary(summary, global_step)\n",
    "\n",
    "## used to generate log directory\n",
    "currentDT = datetime.now()\n",
    "current_time = str(currentDT).replace(\" \", \"-\")[:-10]\n",
    "print(current_time)\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('logs/' + case_name + case_suffix + '_ep' + str(epoch) + '_tr' + str(n_train_run) + '_' + current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct computation graph (only necessary for `Tensorflow 1.x`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "opt = Adam(lr=learning_rate)\n",
    "\n",
    "trainable_weights = encoder.trainable_weights + decoder.trainable_weights + transition.trainable_weights\n",
    "\n",
    "updates = opt.get_updates(loss, trainable_weights)\n",
    "\n",
    "iterate = K.function([xt, ut, xt1, m_tf, prod_loc_tf, dt], [loss, loss_rec_t, loss_rec_t1, loss_l2_reg, loss_trans, loss_flux_t, loss_flux_t1, loss_prod_bhp_t, loss_prod_bhp_t1], updates=updates)\n",
    "\n",
    "eval_loss = K.function([xt, ut, xt1, m_tf, prod_loc_tf, dt], [loss])\n",
    "\n",
    "num_batch = int(num_train/batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epoch):\n",
    "    for ib in range(num_batch):\n",
    "        ind0 = ib * batch_size\n",
    "        state_t_batch  = state_t_train[ind0:ind0+batch_size, ...]\n",
    "        state_t1_batch = state_t1_train[ind0:ind0 + batch_size, ...]\n",
    "        bhp_batch      = bhp_train[ind0:ind0 + batch_size, ...]\n",
    "        m_batch        = m[ind0:ind0 + batch_size, ...]\n",
    "        dt_batch       = dt_train[ind0:ind0 + batch_size, ...]\n",
    "        \n",
    "        output = iterate([state_t_batch, bhp_batch, state_t1_batch, m_batch, prod_loc, dt_batch])\n",
    "        \n",
    "        n_itr = e * num_train + ib * batch_size + batch_size\n",
    "        write_summary(output[0], 'train/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[1]+output[2], 'train/sum_rec_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[5]+output[6], 'train/sum_flux_loss', summary_writer, n_itr) # log for tensorboard\n",
    "        write_summary(output[7]+output[8], 'train/sum_well_loss', summary_writer, n_itr) # log for tensorboard\n",
    "\n",
    "        if ib % 50 == 0:\n",
    "            print('Epoch %d/%d, Batch %d/%d, Loss %f, Loss rec %f, loss rec t1 %f, loss kl %f, loss_trans %f, loss flux %f, loss flux t1 %f, prod bhp loss %f, prod bhp loss t1 %f' % (e+1, epoch, ib+1, num_batch, output[0], output[1], output[2], output[3], output[4], output[5], output[6], output[7], output[8]))\n",
    "            eval_loss_val = eval_loss([state_t_eval, bhp_eval, state_t1_eval, m_eval, prod_loc, dt_eval])\n",
    "            write_summary(eval_loss_val[0], 'eval/total_loss', summary_writer, n_itr) # log for tensorboard\n",
    "    \n",
    "    print('====================================================')\n",
    "    print('\\n')\n",
    "    print('Epoch %d/%d, Train loss %f, Eval loss %f' % (e + 1, epoch, output[0], eval_loss_val[0]))\n",
    "    print('\\n')\n",
    "    print('====================================================')\n",
    "\n",
    "encoder.save_weights(output_dir + 'e2c_encoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' \\\n",
    "                     % (num_train, latent_dim, learning_rate, epoch))\n",
    "decoder.save_weights(output_dir + 'e2c_decoder_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' \\\n",
    "                     % (num_train, latent_dim, learning_rate, epoch))\n",
    "transition.save_weights(output_dir + 'e2c_transition_dt_' + case_name + case_suffix + train_suffix + model_suffix + '_nt%d_l%d_lr%.0e_ep%d.h5' \\\n",
    "                        % (num_train, latent_dim, learning_rate, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-1.10.0]",
   "language": "python",
   "name": "conda-env-tf-1.10.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
